# 🚗 基于 OrangePi AIpro 和鸿蒙系统的智能网联小车开发

## 一、项目信息

### 1. 项目名称
本项目名称为 **基于 OrangePi AIpro 和鸿蒙系统的智能网联小车开发**。随着无线通信和人工智能技术的快速发展，利用国产开发板及支持鸿蒙系统的移动终端开发智能网联小车，成为了开发者快速利用国产化软硬件技术掌握车联网开发的一种有效手段。本项目使用OrangePi AIpro开发板和高清摄像头搭建智能网联小车，将为智能小车的研究和开发提供重要的实践平台。
### 2. 方案描述
本项目任务主要包括三部分：  
(1)在华为云平台ModelArts上使用Mindyolo，训练目标识别的模型，并将模型在OrangePi AIpro开发板上进行部署，实现对实时视频流的推理。
(2)使用边缘检测算法提取每一帧图像的边缘信息，实现车道线检测、车道偏移计算等辅助控制功能。
(3)开发鸿蒙应用程序实现控制者通过wifi远程控制智能网联小车运动，并将小车采集的视频数据实时通过wifi传输到鸿蒙终端。远程控制者在终端可以实时观看目标检测、车道保持、车道偏移等结果。

### 3. 时间规划
- **第一阶段（2025/7/1 - 2025/7/15）**  
  学习目标识别及车道检测算法，准备数据集与开发环境，熟悉 OrangePi AIpro 部署及鸿蒙开发。  
- **第二阶段（2025/7/16 - 2025/8/16）**  
  实现目标检测、车道检测与偏移量计算，并部署算法至 OrangePi AIpro，测试性能并优化。  
- **第三阶段（2025/8/17 - 2025/9/10）**  
  小车组件组装与控制测试，实现 Wi-Fi 远程控制和视频推流。  
- **第四阶段（2025/9/11 - 2025/9/18）**  
  开发鸿蒙终端应用，实现远程控制。  
- **第五阶段（2025/9/19 - 2025/9/28）**  
  小车功能联调与测试，完成结项报告。
---

## 二、已完成工作

### 1. 目标识别模型训练
- **技术方案**  
  （1）技术方案
本项目的模型训练基于开源物体检测数据集COCO2017，使用华为云ModelArts训练Mindyolo模型。COCO2017数据集是2017年发布的COCO数据集的一个版本,主要用于COCO在2017年后持有的物体检测任务、关键点检测任务和全景分割任务。COCO2017共包含训练集118287张，验证集5000张，80分类。以下是数据集的目录格式：

 MindSpore 是华为开发的一个全场景AI计算框架，支持端、边、云全场景的 AI 应用开发。MindYOLO作为MindSpore生态的一部分，充分利用了MindSpore 的高性能和灵活性。YOLO系列算法MindYOLO实现了多种YOLO系列算法，包括 YOLOv8、YOLOv7、YOLOX、YOLOv5、YOLOv4和YOLOv3。这些算法在目标检测领域具有广泛的应用，MindYOLO 为这些算法的实现和优化提供了统一的框架。MindYOLO的目标是为研究人员和开发者提供一个灵活且标准化的工具包，以便他们能够重新实现现有的实时目标检测方法，并开发自己的新方法。

- **工作成果**  
  在本次项目中，使用MindYOLO的YOLOv5作为目标识别的模型，以下是YOLO算法的具体实现步骤：
（1）划分图像
YOLO将输入图像划分为一个固定大小的网格。
（2）预测边界框和类别
YOLO算法预测出固定数量（通常为5个或3个）的边界框，每个边界框由5个主要属性描述：边界框的位置（中心坐标和宽高）和边界框包含的目标的置信度，以及预测目标的分类结果。
（3）单次前向传递
YOLO通过一个卷积神经网络（CNN）进行单次前向传递，同时预测所有
边界框的位置和类别。相比于其他目标检测算法，YOLO算法只需要一次前向传递即可完成预测，因此具有更快的速度，
（4）损失函数
YOLO使用多任务损失函数来训练网络，该损失函数包括位置损失、置信
度损失和类别损失。其中位置损失衡量预测边界框和真实边界框之间的位置差异。置信度损失衡量边界框是否正确地预测了目标，并惩罚背景框的置信度。类别损失衡量目标类别的预测准确性。
（5）非最大抑制
在预测的边界框中，可能存在多个相互重叠的框，代表同一个目标。为了消除冗余的边界框，YOLO使用非最大抑制算法，根据置信度和重叠程度筛选出最佳的边界框。
YOLO算法的目标检测的速度非常快，标准版本的YOLO可以每秒处理45张图像，YOLO的极速版本每秒可以处理150帧图像。这就意味着YOLO可以小于25毫秒延迟，实时地处理视频。YOLO算法的迁移能力也比较强，可以快速运用到其他新的领域，完成特定需求的目标识别任务。基于以上的特点，可以将训练好的YOLO算法模型部署至边缘计算设备，在适用于所要实现的目标识别任务同时，还保证识别的高实时性和高准确性。
（2）工作成果
华为ModelArts 是一个一站式的AI开发平台，它提供了从数据准备到模型训练、部署等一系列的功能。下面是使用华为ModelArts的基本步骤：登录华为云控制台，在华为云控制台中，找到“人工智能”分类下的“ModelArts”服务，点击ModelArts图标进入ModelArts控制台。ModelArts需要使用华为云的对象存储服务（OBS）来存储数据集。在ModelArts控制台中，选择“数据管理”，然后点击“创建OBS桶”。
由于容量限制，以下是在OBS Browsers+中创建的数据集：

在NoteBook中打开JupyterLab，将数据集和代码文件解压，执行训练文件。
训练结束后，调用mindyolo开发套件的export接口进行模型转换，将训练好的ckpt文件转为onnx模型文件。然后针对OrangePi AIpro板卡，在ATC转换模型完成后，即可得到适用于模型推理的om模型文件。
### 2. 车道线检测
- **技术方案**  
  边缘检测算法是图像处理中用于识别图像中物体边界的关键技术，主要包括Canny、Sobel、Prewitt、Laplacian等经典方法。‌其中，因为‌Canny算子低错误率、高定位精度和双阈值处理等特点，‌Canny算子被认为是最优且最常用的边缘检测算。因此，在本次项目中的车道线检测使用‌Canny边缘检测算法，它能够通过提取每一帧图像的边缘信息，准确地检测到图像中的边缘，以突出最可能的车道线轮廓，同时过滤掉边缘检测图像中非车道部分的边缘信息，从而集中检测车辆前方的车道线。
以下是Canny边缘检测算法的具体构成：
（1）灰度化处理
将输入的彩色图像转换为灰度图像。
（2）高斯滤波
使用高斯滤波器对灰度图像进行平滑处理，以减少噪声的影响。
（3）计算图像梯度
使用Sobel算子计算灰度图像在水平和垂直方向的梯度值，从而得到每个像素点的梯度强度和方向。
（4）非极大值抑制
对于每个像素点，比较其梯度方向上的邻近两个像素点之梯度值的大小，只保留梯度方向上梯度值极大的像素点，有利于消除边缘上的模糊效果。
（5）双阈值处理
将像素点分为三类：强边缘、弱边缘和非边缘。当像素点的梯度值高于较高的阈值时，将其分类为强边缘。当像素点的梯度值低于较低的阈值时，将其分类为非边缘。当像素点的梯度值介于较高和较低的阈值之间时，将其分类为弱边缘。
（6）边缘连接
将弱边缘和其周围的强边缘连接起来形成完整的边缘。
Canny边缘检测算法具有高精度、噪声抑制和边缘连通性等优点，适用于本次智能网联小车的高质量边缘检测任务。在实际的应用中，还需要根据具体情况调整参数和优化算法，以达到最佳的车道线检测的效果。

- **工作成果**  
  以下是车道线检测代码的主要实现步骤：首先将输入图像转为灰度图，并通过高斯模糊降低噪声，然后使用Canny算法提取图像中的边缘。接着，定义了一个梯形的感兴趣区域（ROI），只保留道路所在区域的边缘信息，以避免检测到无关的边缘。最后，通过概率霍夫直线变换在ROI内寻找直线，并根据斜率将其分为左车道线（斜率为负）和右车道线（斜率为正）。为了减少检测结果的抖动，程序使用队列保存最近几帧的车道线数据，并对其进行平滑处理。最后，在图像上绘制平滑后的左右车道线，从而得到稳定的车道线检测结果。以下是效果展示展示：
  可以在以上的图中看到智能车识别到了灰色路面，使用两条红色边缘线将线路的两侧进行了标记。
### 3. 车道偏移量计算
- **技术方案**  
 （1）技术方案
车道偏移量的计算是整个车道保持与方向控制逻辑的核心部分，它直接决定了车辆应该执行“向左微调（AL）”、“向右微调（AR）”还是“直行（F）”的指令。其基本原理是：通过检测图像中的左右车道线位置，估计出车道中心位置，再将其与车辆在图像中的中心点进行比较，二者的差值就是车辆偏离车道中心的程度，即车道偏移量，车道偏移量数值越大，说明偏离越严重，向左边偏移数值为负值，向右边偏移数值为正值。
- **工作成果**  
  车道偏移量的计算本质上是车道中心和车辆中心”的差值。通过检测车道线并求其中点位置，可以近似估计车道的中心线；与车辆中心对比得到偏移量；再依据阈值和符号决定车辆向左、向右还是保持直行。下图为车辆面临向右转弯时，车辆向终端发送偏移量数值为32，提示“AR”指令即右转弯。
  这种算法在直路和轻微弯道上效果较好，能快速反映车辆是否偏离车道，并给出相应调整指令。在指令生成后，智能车会通过UDP将车道偏移量以及转向指令发送到遥控的鸿蒙终端侧。 

### 4. OrangePi AIpro 模型部署
- **技术方案**  
 模型推理整体过程可以分为以下几个阶段：
（1）输入预处理：从摄像头获取原始帧。模型要求的输入数据为RGB格式图片，分辨率为 640×640，输入形状为（1，3，640，640），也即（batchsize，channel，height，width）。在接收到原始图像后使用 letterbox 将图像缩放到模型要求的固定输入大小（如 640×640），同时保持纵横比。将BGR转换为RGB，并进行维度变换（HWC → CHW），并转换为float32打包成张量输入。
（2）模型推理：调用 InferSession 的 infer() 方法，把处理好的图像送入推理引擎。昇腾 NPU 会在硬件层面运行网络的卷积、特征提取和预测过程，输出一个候选框集合，每个候选框包含 [x1, y1, x2, y2, 置信度, 类别]。
（3）后处理：从输出中解析出检测框坐标，并在原始图像上画出检测框。使用NMS（非极大值抑制）去除重叠过高的框，保留最优预测框。调用 scale_coords 将缩放后的坐标映射回原始图像大小。
（4）结果可视化：将图片显示在界面或保存到文件。使用PIL在图像上绘制边界框和中文标签（类别名称和置信度）。将检测后的画面返回给主程序，用于叠加车道线、显示和传输。

图6
使用OrangePi AIpro做目标检测时，需要使用AscendCL接口来调用和释放计算资源。模型的具体部署方式为以下几个步骤：
（1）模型转换：模型的部署采用了昇腾AI推理框架和OM模型文件，原始的MindYOLO 模型经过ONNX导出，再通过昇腾ATC工具转换成OM格式，以便在Ascend 310/310B芯片上运行。
（2）导入需要的第三方库以及调用AscendCL接口推理所需文件，定义模型相关变量，如设备ID，内存申请策略等。
（3）定义模型资源相关基类，承担初始化模型资源、创建输入输出数据集、执行推理、解析输出、释放模型资源等功能，之后的Yolo模型推理可继承此类。
（4）定义yolo模型的具体推理功能，包含前处理、推理、后处理等功能。YoloV5继承自3中的Model，并重写其推理接口（infer函数）.得到模型推理输出结果每一帧图像都会被送入 sess.infer([tensor])，在硬件上完成推理，返回推理结果相比 CPU/GPU，NPU 的优势是延迟低、能耗低，适合边缘计算场景（如智能小车）。
（5）目标分类：模型最后会给出每个候选框的类别概率，并结合置信度筛选最可信的结果。
（6）结果发送：检测结果会被打包成JSON通过UDP发送（如检测到“人:2, 车:1”）。

- **工作成果**  
在模型部署阶段，使用的是前面训练好的MindYOLO系列的目标检测模型（已经转换为昇腾AI芯片可运行的OM 格式）。以下是模型部署好后的目标识别功能展示，画7中为成功识别到了行人和小汽车并给出了识别的置信度。另外，在界面中设计了UDP文字信息显示框，用于实时显示小车与上位机之间的通信数据，如图8所示显示了识别到行人数量，这进一步提升了调试和控制过程的透明性与可追溯性。
### 5. 基于鸿蒙的 UI 界面开发
- **技术方案**  
 （1）技术方案
本次项目使用华为推出的DevEco Studio开发工具，通过ArkTS语言编写上位机用户控制界面，使得支持HarmonyOS系统的手机或平板电脑上的应用开发更加简单快捷。
（1）ArkTS编程开发语言语言
ArkTS是鸿蒙生态应用的开发语言，使用.ets作为ArkTS语言源码文件后缀。提供了声明式UI、状态管理等相应的能力，让开发者以更简洁、更自然的方式开发高性能应用。ArkTS的重要特性之一是静态类型。相比于TS只在编译时进行类型检查，ArkTS将编译时所确定的类型应用到运行性能优化中。由于在编译时就可以确定对象布局，对象属性的访问可以更加高效。面向未来，ArkTS会结合应用开发以及运行时的需求持续演进，引入包括并行和并发能力增强、类型系统增强等方面的语言特性，进一步提升ArkTS应用的开发和运行体验。
（2）鸿蒙系统
HarmonyOS整体遵从分层设计，从下向上依次为：内核层、系统服务层、框架层和应用层。系统功能按照“系统 > 子系统 > 功能/模块”逐级展开，在多设备部署场景下，支持根据实际需求裁剪某些非必要的子系统或功能/模块。HarmonyOS技术架构如下图所示：
- **工作成果**  
目前根据本次开源项目的基本要求，基于DevEco Studio开发工具，使用 ArkTS语言设计并实现了一个功能完善的上位机控制界面。该界面整体布局简洁直观，功能模块清晰，能够满足对智能小车的全方位控制与信息交互需求。具体功能包括：
1.运动控制模块：设计了前进、后退、左转、右转四个基础控制按钮，便于用户直观操控小车的运动状态。
2.舵机控制模块：提供舵机左右、上下的独立调节按钮，实现对摄像头或机械结构的灵活转动控制，扩展了小车在监控和环境感知方面的能力。
3.速度调节模块：在界面中预设高、中、低三档电机速度选项，方便用户根据不同场景需求快速切换小车的运动模式。
4.视频回传模块：集成了实时视频回传画面，用户可以通过上位机界面直接获取小车前端摄像头传回的环境信息，实现远程监控与决策。
5.信息交互模块：在界面中设计了UDP文字信息显示框，用于实时显示小车与上位机之间的通信数据，提升了调试和控制过程的透明性与可追溯性。
界面如下图所示：
### 6. 智能网联车构造设计和模拟沙盘
- **技术方案**  
 智能小车的外观设计以轻量化、模块化和稳定性为主要原则。车体部分采用双层亚克力设计，既保证了承重能力，又便于模块化扩展。在车体上方，搭载有高清摄像头模组，位于车头正前方位置，保证了图像采集的视野开阔和车道线检测的准确性。摄像头安装于舵机上面，可调节角度，以适应不同实验环境下的最佳拍摄角度。车体中部安装有主控板（OrangePi AIpro），负责运行深度学习推理与控制逻辑。车体下层则安装电机驱动模块、电机、电池组等，提供前进、后退、转向等基础动力，电池组较重安装在底部可以保持车体重心稳定。
驱动系统采用四轮差速驱动，其中前后轮由直流电机通过减速齿轮带动，配合麦克纳木轮，能够适应多种路面环境。电机与车架通过金属支架紧固，既增强了车体强度，又减少了振动对摄像头和传感器的干扰。
以下是智能车的整体搭建图：

图10

图11
整体搭建过程中，各功能模块通过标准化接口和螺栓固定，保证了小车结构的可维护性与可扩展性。同时，车体表面留有足够空间，以便后续安装5G通信模块、额外传感器或散热装置，从而满足不同实验场景的需求。
本次实验搭建了一个供智能网联小车行驶的模拟沙盘。整体线路由灰色贴纸铺设而成，用以模拟实际道路环境，保证小车能够按照既定路线稳定行驶。在道路两侧布置了人形与车辆模型，模拟真实交通场景中的行人和过往车辆，以增加环境复杂性和测试真实性。通过该沙盘环境，小车在运行过程中不仅能够进行基本的行进和转向控制，还能够结合目标识别、车道线检测等功能实现对“行人”和“车辆”的识别与行驶指令发送，从而验证智能网联技术在真实道路条件下的应用效果。以下是实验场景图：

- **工作成果**  
  完成智能车结构搭建和沙盘模拟实验，验证目标识别、车道线检测及车道保持功能。

---

## 三、遇到的问题及解决方案
1. **MindYOLO 模型 ONNX 导出**  
   - 问题：SiLU 算子不被支持。  
   - 解决方案：当使用mindyolo开发套件将训练好的ckpt文件转为onnx等模型文件的时候，可能会遇到Mindspore不支持SiLU算子的情况，这个时候可以参考https://www.hiascend.com/document/detail/zh/Atlas200IDKA2DeveloperKit/23.0.RC2/Appendices/ttmutat/atctool_errorcode_0170.html。。

2. **AscendCL 接口端侧推理**  
   - 问题：未初始化 AscendCL 会导致资源错误。  
   - 解决方案：在调用AscendCL相关资源时，必须先初始化AscendCL，否则可能会导致后续系统内部资源初始化出错。读入图片，调用model.infer进行推理，其中包含数据的前处理、输入数据集结构的创建、推理、将推理结果转换为numpy、并进行后处理等操作，得到最终带有检测框的图片结果，最后将结果保存到图片。最后记得释放相关资源，包括卸载模型、销毁输入输出数据集、释放 Context、释放指定的计算设备、以及AscendCL去初始化等操作。

3. **智能车实时视频推流延迟**  
   - 问题：HTTP 请求-响应模式导致延迟高。  
   - 解决方案：于延迟较高的问题，是因为HTTP 协议基于请求-响应模式，不是为实时流媒体设计的，推流过程中经常出现 1~3 秒甚至更高的延迟。通过降低视频分辨率、减少帧率（例如从 30fps 调整到 15fps）可以显著降低延迟。同时在局域网环境下比公网更稳定。在Python Flask或Django等框架中实现 HTTP 推流时，单线程下并发连接数量有限，当多个客户端同时请求时，容易出现阻塞或延迟增加。使用多线程或异步框架（如FastAPI、Tornado）提升并发能力。对视频帧进行统一缓存，多个客户端共享同一帧，避免重复解码。

4. **鸿蒙控制界面开发问题**  
   - 问题：ArkTS 框架、网络通信、性能优化适应。  
   - 解决方案：在鸿蒙控制界面开发过程中，主要遇到的问题集中在编程框架适应、网络通信以及性能优化上。由于鸿蒙采用 ArkTS 和声明式 UI，与传统 Android开发存在较大差异，初期在布局编写和事件绑定上有一定学习成本；在与小车端的通信中，HTTP 请求延迟较高，WebSocket 适合用于实时控制，但实现过程中需要解决长连接稳定性和权限配置问题；在性能方面，视频推流和控制逻辑若在同一线程执行容易造成界面卡顿，需要通过异步机制进行解耦。整体心得是：鸿蒙界面开发在交互逻辑和多端适配上具有优势，但需要开发者熟悉其新框架特性，并合理选择通信协议和架构设计，才能实现流畅、稳定的远程控制体验。
---

## 四、后续工作安排
1. **多传感器融合**  
   - 在现有摄像头基础上，增加激光雷达用于精确测距和建图，超声波传感器用于近距离障碍物检测。

2. **自动驾驶辅助功能**  
   - 自动巡航：利用激光雷达实现前车距离保持与速度控制。自动刹车/避障：通过多传感器检测障碍物并实时计算最优制动策略。

3. **智能路径规划与导航**  
   - 利用Dijkstra等算法进行全局路径规划，结合实时传感器数据进行局部路径调整，实现动态避障。

4. **远程监控与控制**  
   - 搭建小车端与云端通信模块（5G/Wi-Fi），实现数据上传和远程指令下发，在移动端提供实时视频流、状态数据和手动控制界面。
